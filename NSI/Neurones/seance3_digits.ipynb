{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af85cbf0",
   "metadata": {},
   "source": [
    "# TP Réseaux de Neurones – Séance 3\n",
    "## Reconnaissance de chiffres manuscrits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0b64cc",
   "metadata": {},
   "source": [
    "### Objectifs\n",
    "- Charger un dataset de chiffres manuscrits.\n",
    "- Implémenter un petit réseau de neurones (64 -> 32 -> 10).\n",
    "- L’entraîner et évaluer sa performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31106a43",
   "metadata": {},
   "source": [
    "## 1. Chargement des données"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71fd0357-8909-4110-8d93-7c5a984c5939",
   "metadata": {},
   "source": [
    "On va utiliser la bibliothèque Scikit Learn (https://scikit-learn.org/stable/) spécialisée dans l'apprentissage machine. Elle contient de nombreuses fonctionnalités pour les réseaux de neurones mais aussi des jeux de tests, dont celui que noua allons utiliser : l'écriture chiffrée manuscrite.\n",
    "\n",
    "Il s'agit d'images 8x8 en niveaux de gris d'écritures variées des chiffres de 0 à 9. On a environ 180 variantes d'écritures par classes (chiffres) poru un total de 1797 données. Chacun des 64 pixels est codé avec une valeur entière de 0 à 15 représentant un des 16 niveaux de gris.\n",
    "\n",
    "Après avoir chargé tout le jeu de données, nous utiliserons une fonction qui va le séparer en deux jeux : un jeu de données qui seriva d'entrainement du réseau et un jeu (inconnu du réseau) que l'on utilisera pour tester la capacité de reconnaissance du réseau.\n",
    "\n",
    "Mais avant quelques remarques :\n",
    "- Normalisation : comme souvent en optimisation, pour limiter l'influence de la taille des valeurs numériques, on les normalise afin d'avoir des valeurs dans un même intervalle, souvent [0,1].\n",
    "- encodage one-hot : ici les étiquettes des données correspondent au chiffre effectivement représenté sur l'image, c'est à dire des valeurs de 0 à 9. Ce choix pourrait induire pour le réseau une notion d'ordre ou de métrique. C'est une des raisons pour lesquelles on utilise un encodage sous forme de vecteur de la manière suivante. Si on a K caractéristiques au total, on utilise un vecteur binaire (0 ou 1) de dimension K. Par exemple ici pour dire que l'image correspond au chiffre 3 (sachant qu'il y en a 10 au total), on va coder l'éthiquette par le vecteur [0,0,0,1,0,0,0,0,0,0]. Cet encodage ne marche que pour des valeurs discrètes.\n",
    "- Dense / creuse : l'inconvénient du One-Hot est qu'on a des vecteurs et matrices creuses (beaucoup de 0) ce qui surcharge la mémoire pour rien. Heureusement on peut demander à ce que l'encodeur One-Hot retourne un tableau dense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dabb2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On importe la fonction qui charge le jeu de données Digits \n",
    "# (images 8×8 de chiffres manuscrits 0–9). \n",
    "# Ce dataset contient 1797 images en niveaux de gris\n",
    "\n",
    "from sklearn.datasets import load_digits\n",
    "\n",
    "# On importe l’outil pour séparer le jeu de données en train \n",
    "# et test de façon aléatoire mais contrôlée.\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# On importe l’encodeur pour transformer les étiquettes entières \n",
    "# (0…9) en vecteurs one-hot de dimension 10 (utile pour les \n",
    "# réseaux avec softmax + cross-entropy).\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Les bibliothèques de calcul et d'affichage\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# On charge le dataset dans un objet de type Bunch \n",
    "# (un peu comme un dict).\n",
    "# Propriétés principales :\n",
    "# - digits.data : tableau (1797, 64) — chaque image 8×8 aplatie en 64 features.\n",
    "# - digits.images : tableau (1797, 8, 8) — images 2D originales.\n",
    "# - digits.target : vecteur (1797) — labels entiers de 0 à 9.\n",
    "\n",
    "digits = load_digits()\n",
    "\n",
    "# digits.data contient des intensités entre 0 et 16 (inclus).\n",
    "# On normalise en divisant par 16 pour obtenir des valeurs dans [0, 1].\n",
    "# Shape : X est (1797, 64).\n",
    "\n",
    "X = digits.data/16.0\n",
    "\n",
    "# digits.target est 1D (1797,) ; \n",
    "# OneHotEncoder attend une entrée 2D (colonnes = features).\n",
    "# reshape(-1, 1) transforme en colonne : (1797, 1).\n",
    "# (Le -1 demande à NumPy d’inférer automatiquement le nombre de lignes.)\n",
    "\n",
    "y = digits.target.reshape(-1,1)\n",
    "\n",
    "# On crée l’encodeur one-hot.\n",
    "# sparse_output=False ⇒ on veut un tableau dense NumPy \n",
    "# (et non une matrice clairsemée).\n",
    "\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "\n",
    "# On apprend les catégories présentes (0…9) et \n",
    "# transforme y en vecteurs one-hot.\n",
    "# Shape : y_onehot est (1797, 10).\n",
    "# Exemple :\n",
    "#       label 3 → [0,0,0,1,0,0,0,0,0,0]\n",
    "#       label 9 → [0,0,0,0,0,0,0,0,0,1]\n",
    "\n",
    "y_onehot = encoder.fit_transform(y)\n",
    "\n",
    "# On sépare les données en 80% train / 20% test.\n",
    "# random_state=0 rend la découpe reproductible.\n",
    "# Shapes attendues :\n",
    "#     X_train : (1437, 64)\n",
    "#     X_test : (360, 64) (car 20% de 1797 ≈ 360)\n",
    "#     y_train : (1437, 10)\n",
    "#     y_test : (360, 10)\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y_onehot,test_size=0.2,random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ae4eeb",
   "metadata": {},
   "source": [
    "## 2. Fonctions utiles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c7051e-f3be-44bb-bc0b-4fd7dc91cb7b",
   "metadata": {},
   "source": [
    "On va réaliser un réseau de neuronnes avec 64 entrées (une par pixel d'image), une couche cachée de 32 neurones et une couche de sortie de 10 neurones, chacun fournissant une sortie 0 ou 1. L'ensemble des 10 sorties fournira le codage One-Hot de l'éthiquette reconnue.\n",
    "\n",
    "Pour la couche cachée on va utiliser comme fonction d'activation la fonction sigmoïde déjà utilisée dans les TP précédents.\n",
    "\n",
    "Par contre, pour la couche de sortie on va utiliser une nouvelle fonction d'activation : la fonction softmax. C'est la fonction la plus utilisée en couche de sortie. Elle renvoie la distribution de probabilité sur des classes mutuellement excusives.\n",
    "\n",
    "Si $z\\in R^{N}$ : $$\\forall i\\in\\{1,...,N\\}, softmax(z)_i = \\frac {e^{z_i}}{\\sum_{j=0}^{N} e^{z_j}}$$\n",
    "Chaque sortie est dans [0,1] et la somme fait bien 1.\n",
    "\n",
    "Elle a deux caractéristiques :\n",
    "- Elle amplifie les grandes valeurs $z_i$ (probabilité plus grande)\n",
    "- Elle est invariante par décalage : $\\forall c\\in R, softmax(z+c)=softmax(z)$ ce qui permet de la stabiliser numériquement en prenant $c=-max(z_i)$. En effet le calcul de l'exponentielle avec des grandes valeurs pourrait provoquer un overflow.\n",
    "\n",
    "Si on note p=softmax(z), sa jacobienne est :$$\\frac{\\partial p_i}{\\partial z_j}=p_i.(\\delta_{ij}-p_j)$$\n",
    "où $\\delta_{ij}=0$ si $i=j$ et 0 sinon.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e82a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1/(1+np.exp(-z))\n",
    "\n",
    "def softmax(z):\n",
    "    expz=np.exp(z-np.max(z,axis=1,keepdims=True))\n",
    "    return expz/np.sum(expz,axis=1,keepdims=True)\n",
    "\n",
    "def cross_entropy(y_true,y_pred):\n",
    "    eps=1e-9\n",
    "    return -np.mean(np.sum(y_true*np.log(y_pred+eps),axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d885cd-cef7-4d2e-882d-d24f58084df8",
   "metadata": {},
   "source": [
    "## 3. Initialisation des poids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82eb30c0-f794-4bdf-a565-4db68a605f71",
   "metadata": {},
   "source": [
    "Pour la couche cachée, W1 et b1, et pour la couche de sortie, W2 et b2. Les 32 neurones de la couche cachée reçoivent les 64 entrées avec un poids par entrée, d'où une matrice (64,32). Les 10 neurones de la couche de sortie reçoivent en entrée la sortie des 32 neurones de la couche cachée d'où une matrice (32,10)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41e52f0-8781-4f3f-bc90-9deb0a42b585",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "W1=np.random.randn(64,32)*0.01\n",
    "b1=np.zeros(32)\n",
    "W2=np.random.randn(32,10)*0.01\n",
    "b2=np.zeros(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a039c663",
   "metadata": {},
   "source": [
    "## 4. Forward et backward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb59f02a-6e83-4b26-8365-3f6ab4fc6725",
   "metadata": {},
   "source": [
    "Pour le produit matriciel, on pourra utiliser l'orpérateur A @ B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f99198",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : coder forward et backward pour MLP\n",
    "def forward(X,W1,b1,W2,b2):\n",
    "    # Calcul de la sortie de la couche cachée\n",
    "    z1=X@W1+b1\n",
    "    a1=sigmoid(z1)\n",
    "    # Calcul de la sortie de la couche de sortie\n",
    "    z2=a1@W2+b2\n",
    "    a2=softmax(z2)\n",
    "    return a1,a2\n",
    "\n",
    "def backward(X,y,a1,a2,W2):\n",
    "    m=len(y)\n",
    "    dz2=a2-y\n",
    "    dW2=(a1.T @ dz2)/m\n",
    "    db2=np.mean(dz2,axis=0)\n",
    "    dz1=(dz2 @ W2.T)*a1*(1-a1)\n",
    "    dW1=(X.T @ dz1)/m\n",
    "    db1=np.mean(dz1,axis=0)\n",
    "    return dW1,db1,dW2,db2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a9efc4",
   "metadata": {},
   "source": [
    "## 4. Entraînement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8b1d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=0.1\n",
    "iteMax=2000\n",
    "pertes=[]\n",
    "\n",
    "for ite in range(iteMax):\n",
    "    a1,a2=forward(X_train,W1,b1,W2,b2)\n",
    "    l=cross_entropy(y_train,a2)\n",
    "    pertes.append(l)\n",
    "    dW1,db1,dW2,db2=backward(X_train,y_train,a1,a2,W2)\n",
    "    W1-=lr*dW1; b1-=lr*db1\n",
    "    W2-=lr*dW2; b2-=lr*db2\n",
    "\n",
    "plt.plot(pertes)\n",
    "plt.title(\"Évolution de la perte\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c839fd",
   "metadata": {},
   "source": [
    "## 5. Évaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e4a9e4-b084-4848-b35d-bea7fe1568be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d5bf1924-0a2b-463c-99fc-639be14b0045",
   "metadata": {},
   "source": [
    "Nous allons maintenant évaluer l'efficacité du réseau à reconnaître les chiffre présents sur un jeu de données. Pour cela, on calcule les sorties pour un jeu de données (X,y) en entrée. \n",
    "\n",
    "Par exemple, si l'image du chiffre 2 est donnée en entrée, on devrait avoir en sortie la réponse y=(0,0,1,0,0,0,0,0,0,0), soit la plus grosse composante de y  en indice 2 (argmax(y)). Il s'agit de la réponse attendue. Si le réseau répond $\\hat y = (0.01,0.01,0.91,0.01,0.01,0.01,0.01,0.01,0.01,0.01)$, cela indique que la plus grosse probabilité en sortie est en indice 2 (argmax($\\hat y$)).\n",
    "\n",
    "Le fait que, sur cette image, argmax(y)==argmax($\\hat y$) indique que le réseau a bien reconnu le chiffre de l'image. Je rapelle qu'en Python l'expression argmax(y)==argmax($\\hat y$) a la valeur 1 (synonyme de True). Il suffit donc de faire la moyenne de ces comparaisons pour l'ensemble du jeu de données fournies au réseau. Une moyenne de 0.9 indiquerait que le réseau reconnait dans 9 cas sur 10.\n",
    "\n",
    "Un apprentissage n'étant pas parfait, on va comparer la réussite sur le jeu d'entrainement (où le score devrait être élevé) avec celle sur le jeu de test (inconnu du réseau). Si les taux sont comparables on pourra dire que le réseau est assez efficace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6a3c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_reseau(X,y,W1,b1,W2,b2):\n",
    "    a1,a2=forward(X,W1,b1,W2,b2)\n",
    "    return np.mean(np.argmax(a2,axis=1)==np.argmax(y,axis=1))\n",
    "\n",
    "print(\"Eval jeu d'apprentissage :\",evaluation_reseau(X_train,y_train,W1,b1,W2,b2))\n",
    "print(\"Eval jeu de test :\",evaluation_reseau(X_test,y_test,W1,b1,W2,b2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dac339f-00a6-4eeb-9d98-932d20effef2",
   "metadata": {},
   "source": [
    "Voyons sur un exemple particulier du jeu de test :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0c23a1-fd94-4edd-969a-1000f59558f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choisir un indice dans le jeu test (ex. aléatoire)\n",
    "idx = np.random.randint(X_test.shape[0])\n",
    "# idx = 0  # ou fixez un index si vous préférez\n",
    "\n",
    "# 1) Prédiction du réseau pour cet exemple\n",
    "_, proba = forward(X_test[idx:idx+1], W1, b1, W2, b2)  # (1,10)\n",
    "pred_label = int(np.argmax(proba, axis=1)[0])\n",
    "true_label = int(np.argmax(y_test, axis=1)[idx])       # y_test est one-hot\n",
    "\n",
    "# 2) Repasser du vecteur (64,) à l'image 8x8\n",
    "img8 = X_test[idx].reshape(8, 8)\n",
    "\n",
    "# 3) agrandissement 8x8 -> 64x64\n",
    "#    np.kron répète chaque pixel 8x8 fois\n",
    "img64 = np.kron(img8, np.ones((8, 8)))\n",
    "\n",
    "# 4) Affichage\n",
    "plt.figure(figsize=(4, 4))\n",
    "plt.imshow(img64, cmap='gray', vmin=0, vmax=1)\n",
    "plt.title(f\"Prédit: {pred_label}  |  Vrai: {true_label}\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anaconda-2025.06-py3.11",
   "language": "python",
   "name": "conda-env-anaconda-2025.06-py3.11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
