{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57d7ba8a",
   "metadata": {},
   "source": [
    "# Régression Linéaire"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b52d5f",
   "metadata": {},
   "source": [
    "## Objectifs\n",
    "- Comprendre le modèle linéaire\n",
    "- Implémenter la descente de gradient\n",
    "- Visualiser la droite de régression\n",
    "- Expérimenter avec des données concrètes (Taille, poids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2c2d69-0f93-4afa-9d55-f72f974af442",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "La régression linéaire est une méthode statistique très ancienne qui a été reprise dans le Machine Learning. Son principe est simple. Supposons qu'on dispose d'un ensemble de données de type nuage de points (x,y) :\n",
    "- on cherche une fonction linéaire (modélisation) qui décrit la relation entre x et y\n",
    "- à partir de ce modèle, on peut prédire des valeurs de y pour un x donné et ne figurant pas dans l'ensemble de données de départ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c415f1bf",
   "metadata": {},
   "source": [
    "## Jeu de données\n",
    "On va supposer que le poids (kg) d'un individu dépend de la taille (cm) de manière approximativement linéaire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ce1652-3ef2-4347-8b07-33c2985a8693",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# On fixe la \"graine\" du générateur pseudo-aléatoire pour la reproductibilité de l'expérience\n",
    "np.random.seed(0)\n",
    "\n",
    "# Génération de 50 tailles (en cm) entre 150 et 200 cm\n",
    "taille = np.random.randint(150, 200, 50)\n",
    "\n",
    "# Génération de poids avec bruit gaussien (poids ~ 0.9*taille - 60)\n",
    "poids = 0.9*taille - 60 + np.random.randn(50)*5\n",
    "\n",
    "# Affichage graphique du nuage de points\n",
    "plt.scatter(taille, poids, color=\"blue\")\n",
    "plt.xlabel(\"Taille (cm)\")\n",
    "plt.ylabel(\"Poids (kg)\")\n",
    "plt.title(\"Relation taille - poids\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b30ab6d",
   "metadata": {},
   "source": [
    "## Modèle linéaire\n",
    "Le modèle est de la forme : $$y = w . x + b$$\n",
    "où :\n",
    "- $w$ est la pente\n",
    "- $b$ est l'ordonnée à l'origine\n",
    "\n",
    "Comme on ne connaît pas à priori les paramètres optimaux du modèle, on prend des valeurs aléatoires (suivant une loi normale centrée réduite) pour $w$ et $b$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b61805",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.random.randn()\n",
    "b = np.random.randn()\n",
    "# Fonction qui prédit y en fonction de x et du modèle linéaire\n",
    "def predict(x,w,b):\n",
    "    return w*x+b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b04bad4",
   "metadata": {},
   "source": [
    "## Fonction de coût\n",
    "La fonction de coût traduit l'erreur commise par le modèle. Pour chaque $x_{reel}$ du jeu de données, on dispose d'un $y_{reel}$ du jeu de donnée et du $y_{predit}$ donné par le modèle, soit $y_{predit} = w.x_{reel}+b$. On peut donc définir une erreur au niveau du point égale à $|y_{predit} - y_{reel}|$. L'erreur globale, ou fonction coût, peut alors être définie par : $$\\frac{1}{n} . \\sum (y_{predit} - y{reel})^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04237ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(y_reel,y_predit):\n",
    "    return np.mean((y_reel-y_predit)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da1ed1b-4aa1-498c-a4be-d0e504d4806c",
   "metadata": {},
   "source": [
    "Le but est maintenant de trouver les paramètres optimaux du modèle, c'est à dire ceux qui minimisent la fonction coût."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb1542f",
   "metadata": {},
   "source": [
    "## Descente de gradient\n",
    "Notre fonction coût peut être vue comme une fonction de $w$ et $b$ : $$F(w,b) = \\frac{1}{n} . \\sum (y_{reel} - (w.x_{reel}=b))^2$$\n",
    "Il s'agit de minimiser cette fonction.\n",
    "### principe\n",
    "La descente de gradient est une méthode itérative pour trouver le minimum d’une fonction.\n",
    "On part d’une valeur initiale des paramètres (au hasard) puis, étape après étape, on déplace les paramètres dans la direction opposée du gradient (car le gradient indique la direction de la plus forte pente ascendante).\n",
    "### Règle de mise à jour\n",
    "Pour chaque paramètre $\\theta$ du modèle (par exemple $w$ ou $b$) : $$\\theta \\leftarrow \\theta - \\eta . \\frac{\\partial F}{\\partial \\theta}$$  \n",
    "où :\n",
    "- $\\eta$ est le taux d'apprentissage (learning rate) : un petit nombre positif qui contrôle la taille du pas.\n",
    "- $\\frac{\\partial F}{\\partial \\theta}$ est la dérivée partielle de la fonction de coût par rapport au paramètre $\\theta$.\n",
    "### Illustration intuitive \n",
    "Imaginez que vous êtes en haut d’une colline dans le brouillard :\n",
    "- Vous voulez atteindre la vallée (le minimum de la fonction de coût).\n",
    "- Vous ne voyez pas le paysage entier, mais vous sentez la pente sous vos pieds.\n",
    "- À chaque pas, vous descendez dans la direction la plus raide (le gradient négatif).\n",
    "- Si vos pas sont trop grands ($\\eta$ trop grand), vous risquez de sauter par-dessus la vallée.\n",
    "- Si vos pas sont trop petits ($\\eta$ trop petit), vous avancez très lentement.\n",
    "### Maintenant à vous ...\n",
    "Commencez par écrire, pour chaque paramètre $w$ et $b$ la formule de mise à jour suivant la méthode de descente de gradient.\n",
    "A partir de là, en prenant $\\eta=10^{-4}$ et un nombre d'itérations de nb_ite = 1000, ecrivez le programme qui calcule le modèle optimal par descente de gradient. On stockera dans une liste les valeurs de la fonction coût à chaque itération.\n",
    "Vous pourrez faire afficher graphiquement :\n",
    "- l'évolution de la fonction coût sur l'ensemble des itérations.\n",
    "- le nuage de points et la droite de régression obtenue à la fin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf2d2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descente de gradient\n",
    "eta = 0.0001\n",
    "nb_ite = 1000\n",
    "erreur = []\n",
    "\n",
    "for i in range(nb_ite):\n",
    "    #...\n",
    "    \n",
    "    # gradients\n",
    "    #dw = ...\n",
    "    #db = ...\n",
    "    \n",
    "    # mise à jour\n",
    "    #w = ...\n",
    "    #b = ...\n",
    "\n",
    "plt.plot(losses)\n",
    "plt.xlabel(\"Itérations\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.title(\"Évolution de l'erreur\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b46600a-ead5-436b-80d0-b2c8f7ec1307",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation de la droite ajustée\n",
    "plt.scatter(taille, poids, color=\"blue\")\n",
    "plt.plot(taille, predict(taille, w, b), color=\"red\", linewidth=2)\n",
    "plt.xlabel(\"Taille (cm)\")\n",
    "plt.ylabel(\"Poids (kg)\")\n",
    "plt.title(\"Régression linéaire : taille -> poids\")\n",
    "plt.show()\n",
    "\n",
    "print(\"w =\", w, \" b =\", b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5101362b-38da-401a-8514-9e0b731d1e62",
   "metadata": {},
   "source": [
    "Vous pouvez faire varier le learning rate et le nombre d'itérations pour voir ce qui se passe."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3-tf]",
   "language": "python",
   "name": "conda-env-anaconda3-tf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
