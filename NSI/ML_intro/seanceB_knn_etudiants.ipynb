{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0553b8fa",
   "metadata": {},
   "source": [
    "# k plus proches voisins (kNN)\n",
    "## Introduction\n",
    "La méthode des k plus proches voisins (kNN pour k-Nearest Neighbors) est une méthode supervisée utilisée en classification et en régression.\n",
    "C’est un algorithme très simple : au lieu de construire un modèle complexe, il mémorise les données d’entraînement et prend sa décision en fonction des exemples les plus proches.\n",
    "## Principe de fonctionnement\n",
    "1. On dispose d'un ensemble de points d'apprentissage (x,y) où chaque $x_i$ est une observation et $y_i$ son label (classe ou valeur).\n",
    "2. Pour prédire la classe d'un nouveau point $x_{new}$ :\n",
    "   - On calcule la distance entre $x_{new}$ et tous les points connus;\n",
    "   - On sélectionne les k plus proches voisins (d'où le nom de la méthode);\n",
    "   - En classification : on fait un vote majoritaire des classes parmi ces voisins;\n",
    "   - En régression : on calcule la moyenne des valeurs des voisins.\n",
    "## Choix des paramètres\n",
    "Valeur de k :\n",
    "- k petit : le modèle est très sensible aux points isolés (risque de surapprentissage), il colle trop aux données (même au bruit).\n",
    "- k grand : le modèle est plus stable mais moins précis sur les frontières fines (effet de lissage important).\n",
    "  \n",
    "Type de distance :\n",
    "- Euclidienne (la plus courante) : $d(x_1,x_2) = \\sqrt {\\sum (x_i - x_i')^2}$\n",
    "- Manhattan : $d(x_1,x_2) = \\sum |x_i - x_i'|$\n",
    "- d'autres distances peuvent être utilisées selon le problème.\n",
    "## Avantage et limites \n",
    "Avantages :\n",
    "- très simple à comprendre et mettre en oeuvre\n",
    "- Pas de phase d'apprentissage complexe\n",
    "- S'adapte bien à des données avec peu de dimensions\n",
    "\n",
    "Inconvénients :\n",
    "- Lent si beaucoup de données (calcul de toutes les distances)\n",
    "- Sensible à l'échelle des variables (nécessite souvent une normalisation)\n",
    "- Sensible au choix de k et de la distance\n",
    "## Utilisations typiques \n",
    "- Reconnaissance d'images ou de chiffres manuscrits\n",
    "- Classification de fleurs\n",
    "- Systèmes de recommandation simples (trouver des utilisateurs \"proches\")\n",
    "\n",
    "## Exemple : Iris (2 classes)\n",
    "### Le Dataset\n",
    "Il s'agit d'un jeu de données très utilisé en statistiques et en Machine Learning, introduit par le statisticien Ronald Fisher en 1936. Il contient 150 échantillons de fleurs d’iris, répartis en 3 espèces :\n",
    "- Iris setosa\n",
    "- Iris versicolor\n",
    "- Iris virginica\n",
    "\n",
    "Chaque fleur est décrite par 4 caractéristiques (features) :\n",
    "- Longueur du sépale (cm)\n",
    "- Largeur du sépale (cm)\n",
    "- Longueur du pétale (cm)\n",
    "- Largeur du pétale (cm)\n",
    "### Réduction à 2 classes et 2 caractéristiques\n",
    "Pour simplifier, on choisit seulement deux espèces :\n",
    "- Iris setosa (classe 0)\n",
    "- Iris versicolor (classe 1)\n",
    "\n",
    "On choisit seulement 2 caractéristiques :\n",
    "- Longueur du sépale (axe x)\n",
    "- Largeur du sépale (axe y)\n",
    "### Exemple de représentation \n",
    "Chaque fleur est représentée par un point :\n",
    "- En bleu : Iris setosa\n",
    "- En rouge : Iris versicolor\n",
    "L'algorithme kNN devra décider pour un nouveau point inconnu (par exemple une fleur dont on mesure la longueur et la largeur de sépale) à quelle classe elle appartient, en fonction de ses voisins les plus proches.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b13799",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Chargement du Dataset contenu par défaut dans la bibliothèque sklearn\n",
    "iris = datasets.load_iris()\n",
    "# iris.data contient les 4 mesures pour chaque fleur, on garde seulement les 2 première colonnes (longueur et largeur du sépale)\n",
    "# iris.target contient les étiquettes (0=setosa, 1=versicolor, 2=virginica)\n",
    "X,y = iris.data[:,:2],iris.target\n",
    "# On enlève la classe 2 = virginica\n",
    "X,y = X[y!=2], y[y!=2]\n",
    "# On trace le nuage de points, la couleur dépendant de la classe donnée par y, palette de couleur du bleu au rouge bwr)\n",
    "plt.scatter(X[:,0],X[:,1],c=y,cmap=\"bwr\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e17af2b-c2b1-460c-9715-97c7aa796d67",
   "metadata": {},
   "source": [
    "Ecrire une fonction qui calcule la distance euclidienne entre les points x1 et x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3658648e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance(x1,x2): ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bfb0ce8-0cd3-4255-9645-c67c3bb58346",
   "metadata": {},
   "source": [
    "Ecrire une fonction qui prédit la classe pour une nouvelle donnée :\n",
    "- $X_{train}$ et $y_{train}$ sont les données du Dataset\n",
    "- $x_{new}$ le point dont on doit prédire la classe\n",
    "- k le nombre de voisins.\n",
    "\n",
    "Il s'agit de :\n",
    "- calculer la distance entre $x_{new}$ et chaque point du jeu d'entraînement\n",
    "- Trier les distances (on pourra utiliser np.argsort de numpy) et garder les k plus petits indices.\n",
    "- Récupérer les labels des k plus proches voisins\n",
    "- Compter combien de fois chaque classe apparaît\n",
    "- Retrouner la classe majoritaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f26a68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_predict(X_train,y_train,x_new,k=3): ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b627fd19-9090-4472-8392-b9f7d59dbbcf",
   "metadata": {},
   "source": [
    "### Visualisation des frontières"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc79579-ebef-4fc9-8a26-1966937a544f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On crée une grille de points couvrant tout le plan\n",
    "h = .1  # pas de la grille\n",
    "x_min, x_max = X[:,0].min()-1, X[:,0].max()+1\n",
    "y_min, y_max = X[:,1].min()-1, X[:,1].max()+1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                     np.arange(y_min, y_max, h))\n",
    "\n",
    "# Pour chaque point de la grille, on prédit sa classe avec kNN\n",
    "Z = np.array([knn_predict(X, y, np.array([xx1,yy1]), k=3)\n",
    "              for xx1,yy1 in zip(xx.ravel(), yy.ravel())])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "plt.contourf(xx, yy, Z, cmap=\"bwr\", alpha=0.3)\n",
    "plt.scatter(X[:,0], X[:,1], c=y, cmap=\"bwr\", edgecolors=\"k\")\n",
    "plt.xlabel(\"Longueur sépale (cm)\")\n",
    "plt.ylabel(\"Largeur sépale (cm)\")\n",
    "plt.title(\"Frontière de décision (kNN, k=3)\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3-tf]",
   "language": "python",
   "name": "conda-env-anaconda3-tf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
